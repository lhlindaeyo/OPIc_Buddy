"""
ì±„íŒ… í™”ë©´ ì»´í¬ë„ŒíŠ¸
"""
import streamlit as st
from utils.speech_utils import display_speech_interface, display_tts_button
from utils.model_loader import generate_response
from utils.opic_test_generator import OPIcTestGenerator
from components.survey import get_user_profile

def show_chat(model_interface):
    """
    ì±„íŒ… í™”ë©´ì„ í‘œì‹œí•©ë‹ˆë‹¤.
    
    Args:
        model_interface: ë¡œë“œëœ AI ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ (ë¡œì»¬/API/RAG ì§€ì›)
    """
    st.title("ğŸ¤– OPIc AI íŠœí„°")
    
    # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ìº¡ì…˜ í‘œì‹œ
    model_type = model_interface.get("type", "unknown") if model_interface else "unknown"
    if model_type == "rag":
        st.caption("ğŸ” RAG + Phi-1.5 ëª¨ë¸ë¡œ OPIc ì „ë¬¸ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™œìš©í•œ ë§ì¶¤í˜• ì˜ì–´ íŠœí„°ë§ì„ ì œê³µí•©ë‹ˆë‹¤!")
        st.success("âœ… **RAG ì§ˆë¬¸ ê°œì„  ì‹œìŠ¤í…œ í™œì„±í™”**: ê¸°ì¡´ ì§ˆë¬¸ì„ ì‚¬ìš©ì ê´€ì‹¬ì‚¬ì— ë§ê²Œ ê°œì„ í•©ë‹ˆë‹¤.")
    elif model_type == "local":
        st.caption("ì„¤ë¬¸ì¡°ì‚¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì¸ ë§ì¶¤í˜• ì˜ì–´ ì¸í„°ë·°ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤!")
    else:
        st.caption("AI íŠœí„°ì™€ í•¨ê»˜ ì˜ì–´ ì—°ìŠµì„ ì‹œì‘í•´ë³´ì„¸ìš”!")

    # OPIc ì‹œí—˜ ì‹œì‘ ë²„íŠ¼ í‘œì‹œ
    _display_opic_test_interface(model_interface)

    # ì‚¬ìš©ì í”„ë¡œí•„ í‘œì‹œ
    _display_user_profile()

    # ìŒì„± ì…ë ¥ ì¸í„°í˜ì´ìŠ¤
    display_speech_interface()
    
    # ìŒì„± ì…ë ¥ì´ ìˆì„ ê²½ìš° ì²˜ë¦¬
    if "user_input" in st.session_state and st.session_state.user_input:
        if st.session_state.get("opic_test_active", False):
            _handle_opic_test_answer(st.session_state.user_input, model_interface)
        else:
            _handle_question(st.session_state.user_input, model_interface)
        # ì²˜ë¦¬ í›„ ì´ˆê¸°í™”
        st.session_state.user_input = ""

    # í…ìŠ¤íŠ¸ ì…ë ¥ì°½
    if st.session_state.get("opic_test_active", False):
        # OPIc ì‹œí—˜ ëª¨ë“œ
        user_input = st.chat_input("ğŸ’¬ ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ì–´ë¡œ)")
        if user_input:
            _handle_opic_test_answer(user_input, model_interface)
    else:
        # ì¼ë°˜ ì±„íŒ… ëª¨ë“œ
        user_input = st.chat_input("ğŸ’¬ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ì–´ ë˜ëŠ” í•œêµ­ì–´)")
        if user_input:
            _handle_question(user_input, model_interface)

    # ëŒ€í™” ë Œë”ë§
    _render_chat_history()

def _display_opic_test_interface(model_interface):
    """OPIc ì‹œí—˜ ì¸í„°í˜ì´ìŠ¤ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
    survey_value_pool = st.session_state.get("survey_value_pool", [])
    
    if not survey_value_pool:
        st.warning("âš ï¸ ì„¤ë¬¸ì¡°ì‚¬ë¥¼ ë¨¼ì € ì™„ë£Œí•´ì£¼ì„¸ìš”.")
        return
    
    # OPIc ì‹œí—˜ ìƒíƒœ ì´ˆê¸°í™”
    if "opic_test_generator" not in st.session_state:
        st.session_state.opic_test_generator = OPIcTestGenerator()
        st.session_state.opic_test_active = False
    
    # í˜„ì¬ ìƒíƒœì— ë”°ë¥¸ UI í‘œì‹œ
    if not st.session_state.get("opic_test_active", False):
        # ì‹œí—˜ ì‹œì‘ ì „
        st.markdown("---")
        st.markdown("### ğŸ¯ OPIc ëª¨ì˜ê³ ì‚¬")
        
        st.info("ğŸ“‹ ì„¤ë¬¸ì¡°ì‚¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§ì¶¤í˜• OPIc 15ë¬¸í•­ì´ ìƒì„±ë©ë‹ˆë‹¤.")
        
        col1, col2, col3 = st.columns([2, 1, 2])
        
        with col1:
            st.markdown("**ì‹œí—˜ êµ¬ì„±:**")
            st.markdown("- **1ë²ˆ**: ìê¸°ì†Œê°œ")
            st.markdown("- **2-4ë²ˆ**: ê¸°ë³¸ ì„¸íŠ¸ (ì„¤ë¬¸ ê¸°ë°˜)")
            st.markdown("- **5-7ë²ˆ**: ëŒë°œ ì„¸íŠ¸")
        
        with col2:
            st.markdown("")  # ì—¬ë°±
            st.markdown("")  # ì—¬ë°±
            if st.button("ğŸš€ OPIc ëª¨ì˜ê³ ì‚¬ ì‹œì‘", type="primary"):
                # ì‹œí—˜ ì´ˆê¸°í™” ë° ì‹œì‘
                st.session_state.opic_test_generator.initialize_test(survey_value_pool)
                st.session_state.opic_test_active = True
                
                # ì²« ë²ˆì§¸ ì§ˆë¬¸ ìë™ ìƒì„± (RAG ì‚¬ìš©)
                first_question = st.session_state.opic_test_generator.get_current_question()
                if first_question:
                    # ì²« ë²ˆì§¸ ì§ˆë¬¸ì€ ê³ ì • ìê¸°ì†Œê°œì´ë¯€ë¡œ RAG ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                    if first_question.get("type") == "intro":
                        _add_opic_question_to_chat(first_question)
                    else:
                        # RAGë¡œ ì§ˆë¬¸ ìƒì„±
                        rag_question = _generate_rag_question(first_question, model_interface)
                        if rag_question:
                            first_question["question"] = rag_question
                        _add_opic_question_to_chat(first_question)
                
                st.rerun()
        
        with col3:
            st.markdown("**ì¶”ê°€ êµ¬ì„±:**")
            st.markdown("- **8-10ë²ˆ**: ì‹¬í™” ì„¸íŠ¸ (ì„¤ë¬¸ ê¸°ë°˜)")
            st.markdown("- **11-13ë²ˆ**: ë¡¤í”Œë ˆì‰ ì„¸íŠ¸")
            st.markdown("- **14-15ë²ˆ**: ì‚¬íšŒ ì´ìŠˆ ì„¸íŠ¸")
    else:
        # ì‹œí—˜ ì§„í–‰ ì¤‘ - ì§„í–‰ìƒí™© í‘œì‹œ
        progress = st.session_state.opic_test_generator.get_progress()
        current_q, total_q = progress
        
        # ì§„í–‰ ë°”
        progress_percent = (current_q - 1) / total_q * 100
        st.markdown("---")
        st.markdown(f"### ğŸ¯ OPIc ëª¨ì˜ê³ ì‚¬ ì§„í–‰ ì¤‘: {current_q-1}/{total_q}")
        st.progress(progress_percent / 100)
        
        # ì‹œí—˜ ì¢…ë£Œ ë²„íŠ¼
        col1, col2, col3 = st.columns([1, 1, 1])
        with col3:
            if st.button("ğŸ›‘ ì‹œí—˜ ì¢…ë£Œ", type="secondary"):
                st.session_state.opic_test_active = False
                st.session_state.chat_history.append({
                    "role": "bot", 
                    "content": "ğŸ **OPIc ëª¨ì˜ê³ ì‚¬ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.** ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!"
                })
                st.rerun()

def _add_opic_question_to_chat(question_data):
    """OPIc ì§ˆë¬¸ì„ ì±„íŒ…ì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    question_type_desc = st.session_state.opic_test_generator.get_question_type_description(question_data["type"])
    
    message = f"""ğŸ¤ **Question {question_data['number']}/15**
{question_type_desc}

{question_data['question']}

*Please answer in English.*"""
    
    st.session_state.chat_history.append({
        "role": "bot",
        "content": message
    })

def _handle_opic_test_answer(answer, model_interface):
    """OPIc ì‹œí—˜ ë‹µë³€ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    # ì‚¬ìš©ì ë‹µë³€ì„ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
    st.session_state.chat_history.append({"role": "user", "content": answer})
    
    # ë‹µë³€ì— ëŒ€í•œ ê°„ë‹¨í•œ í”¼ë“œë°± (ì„ íƒì )
    feedback = _generate_simple_feedback(answer)
    if feedback:
        st.session_state.chat_history.append({"role": "bot", "content": feedback})
    
    # ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì´ë™
    next_question = st.session_state.opic_test_generator.get_next_question()
    
    if next_question:
        # RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ì—¬ Survey Value Pool ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±
        rag_generated_question = _generate_rag_question(next_question, model_interface)
        if rag_generated_question:
            next_question["question"] = rag_generated_question
        
        # ë‹¤ìŒ ì§ˆë¬¸ ì¶”ê°€
        _add_opic_question_to_chat(next_question)
    else:
        # ì‹œí—˜ ì™„ë£Œ
        st.session_state.opic_test_active = False
        completion_message = """ğŸ‰ **ì¶•í•˜í•©ë‹ˆë‹¤! OPIc ëª¨ì˜ê³ ì‚¬ë¥¼ ì™„ë£Œí•˜ì…¨ìŠµë‹ˆë‹¤!**

ì´ 15ë¬¸í•­ì„ ëª¨ë‘ ì™„ë£Œí•˜ì…¨ìŠµë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!

ğŸ” **ì‹œí—˜ ê²°ê³¼ ìš”ì•½:**
- ìê¸°ì†Œê°œ: ì™„ë£Œ 
- ê¸°ë³¸ ì„¸íŠ¸ (ì„¤ë¬¸ ê¸°ë°˜): ì™„ë£Œ 
- ëŒë°œ ì„¸íŠ¸: ì™„ë£Œ 
- ì‹¬í™” ì„¸íŠ¸ (ì„¤ë¬¸ ê¸°ë°˜): ì™„ë£Œ 
- ë¡¤í”Œë ˆì‰ ì„¸íŠ¸: ì™„ë£Œ 
- ì‚¬íšŒ ì´ìŠˆ ì„¸íŠ¸: ì™„ë£Œ 

ê³„ì† ì—°ìŠµí•˜ê³  ì‹¶ìœ¼ì‹œë©´ ë‹¤ì‹œ ì‹œì‘ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!"""
        
        st.session_state.chat_history.append({
            "role": "bot",
            "content": completion_message
        })

def _generate_rag_question(question_data, model_interface):
    """RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ì—¬ Survey Value Pool ê¸°ë°˜ ë§ì¶¤í˜• ì§ˆë¬¸ ìƒì„±"""
    # ê°„ë‹¨í•œ RAG ê¸°ë°˜ ì§ˆë¬¸ ê°œì„  ì‹œìŠ¤í…œ
    if not model_interface or model_interface.get("type") != "rag":
        return None
    
    survey_value_pool = st.session_state.get("survey_value_pool", [])
    if not survey_value_pool:
        return None
    
    # ê¸°ì¡´ ì§ˆë¬¸ì„ RAGë¡œ ê°œì„ 
    original_question = question_data.get("question", "")
    survey_context = ", ".join([item for item in survey_value_pool if not item.startswith("level_")])
    
    # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì§ˆë¬¸ ê°œì„ 
    simple_prompt = f"""Improve this OPIc question to be more personalized based on the user's interests:

Original: {original_question}
User interests: {survey_context}

Improved question:"""
    
    try:
        from utils.model_loader import generate_response
        improved_question = generate_response(model_interface, simple_prompt, max_tokens=80)
        
        if improved_question and len(improved_question.strip()) > 10:
            # ê°„ë‹¨í•œ ì •ì œ
            cleaned = improved_question.strip()
            # ì²« ë²ˆì§¸ ì¤„ë§Œ ì‚¬ìš©
            if '\n' in cleaned:
                cleaned = cleaned.split('\n')[0]
            # "Improved question:" ì œê±°
            if "Improved question:" in cleaned:
                cleaned = cleaned.split("Improved question:")[-1].strip()
            if "Question:" in cleaned:
                cleaned = cleaned.split("Question:")[-1].strip()
            
            # ì›ë˜ ì§ˆë¬¸ê³¼ ë„ˆë¬´ ìœ ì‚¬í•˜ë©´ None ë°˜í™˜
            if cleaned.lower() != original_question.lower() and len(cleaned) > 20:
                return cleaned
            
    except Exception as e:
        st.error(f"RAG ì§ˆë¬¸ ê°œì„  ì˜¤ë¥˜: {e}")
    
    return None
    
    survey_value_pool = st.session_state.get("survey_value_pool", [])
    if not survey_value_pool:
        return None
    
    # Survey Value Poolì„ ë¬¸ë§¥ìœ¼ë¡œ ë³€í™˜
    survey_context = ", ".join(survey_value_pool)
    
    # ì§ˆë¬¸ íƒ€ì…ì— ë”°ë¥¸ RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
    question_type = question_data.get("type", "")
    question_number = question_data.get("number", 0)
    
    rag_prompts = {
        "survey_basic": f"""Based on the user's survey data: {survey_context}

Generate an OPIc-style basic question (Question {question_number}/15) that specifically relates to their interests and background. 

The question should be:
- Conversational and natural
- Appropriate for intermediate English learners
- Directly related to their survey responses
- Encourage detailed personal responses

Question:""",

        "unexpected": f"""Generate an OPIc-style unexpected question (Question {question_number}/15) for an intermediate English learner.

The question should be:
- About general topics (not directly related to their survey: {survey_context})
- Conversational and engaging
- Appropriate difficulty for intermediate level
- Encourage personal opinions or experiences

Question:""",

        "survey_advanced": f"""Based on the user's detailed survey data: {survey_context}

Generate an advanced OPIc-style question (Question {question_number}/15) that explores deeper aspects of their interests.

The question should:
- Be more analytical and thoughtful
- Ask about changes, comparisons, or personal growth
- Relate to their specific interests from the survey
- Require explanation and reasoning

Question:""",

        "roleplay": f"""Generate an OPIc-style roleplay question (Question {question_number}/15) for an intermediate English learner.

The question should:
- Present a practical situation they might encounter
- Be appropriate for their level (not too complex)
- Allow them to demonstrate conversational English
- Be relevant to daily life situations

Situation:""",

        "social_issues": f"""Generate an OPIc-style social issues question (Question {question_number}/15) for an intermediate English learner.

The question should:
- Address current social or cultural topics
- Be appropriate for intermediate level (not too academic)  
- Encourage personal opinions and experiences
- Allow for different perspectives

Question:"""
    }
    
    # ê¸°ë³¸ íƒ€ì… ì¶”ì¶œ
    base_type = question_type.split('_')[0] + '_' + question_type.split('_')[1] if '_' in question_type else question_type
    
    # í•´ë‹¹í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ì„ íƒ
    prompt = rag_prompts.get(base_type, rag_prompts.get(question_type, ""))
    
    if not prompt:
        return None
    
    try:
        # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±
        from utils.model_loader import generate_response
        generated_question = generate_response(model_interface, prompt, max_tokens=100)
        
        # ìƒì„±ëœ ì§ˆë¬¸ ì •ì œ
        if generated_question and len(generated_question.strip()) > 10:
            # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
            cleaned_question = generated_question.strip()
            if cleaned_question.startswith("Question:"):
                cleaned_question = cleaned_question[9:].strip()
            elif cleaned_question.startswith("Situation:"):
                cleaned_question = cleaned_question[10:].strip()
            
            return cleaned_question
            
    except Exception as e:
        st.error(f"RAG ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")
        
    return None

def _generate_simple_feedback(answer):
    """ë‹µë³€ì— ëŒ€í•œ ê°„ë‹¨í•œ í”¼ë“œë°± ìƒì„± (ì„ íƒì )"""
    if len(answer.strip()) < 20:
        return "ğŸ’¡ Try to provide more detailed answers to showcase your English skills better!"
    elif len(answer.strip()) > 200:
        return "ğŸ‘ Great detailed response! Well done!"
    else:
        return "ğŸ‘ Good answer!"

def _display_user_profile():
    """ì‚¬ìš©ì í”„ë¡œí•„ì„ ê°„ë‹¨í•˜ê²Œ í‘œì‹œí•©ë‹ˆë‹¤."""
    profile = get_user_profile()
    
    with st.expander("ë‚˜ì˜ í”„ë¡œí•„ (ì„¤ë¬¸ì¡°ì‚¬ ê²°ê³¼)"):
        st.info(profile)
        
        # Survey Value Pool í‘œì‹œ (ê°œë°œììš©)
        with st.expander("ğŸ› ï¸ Survey Value Pool (Dev)"):
            survey_value_pool = st.session_state.get("survey_value_pool", [])
            if survey_value_pool:
                pool_text = "[\n"
                for i, value in enumerate(survey_value_pool):
                    pool_text += f'  {i}: "{value}"\n'
                pool_text += "]"
                st.code(pool_text)
            else:
                st.code("[]")
        
        # ìƒì„¸ ë°ì´í„°ë„ ë³´ì—¬ì£¼ê¸° (ê°œë°œììš©)
        if hasattr(st.session_state, 'survey_data'):
            with st.expander("ğŸ” ìƒì„¸ ë°ì´í„° (ê°œë°œììš©)", expanded=False):
                survey_data = st.session_state.survey_data
                if survey_data:
                    import json
                    st.code(json.dumps(survey_data, indent=2, ensure_ascii=False), language="json")
                else:
                    st.info("ì„¤ë¬¸ì¡°ì‚¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def _handle_question(question, model_interface):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  AI ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´ë¥¼ í¬í•¨í•œ ë§ì¶¤í˜• ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        question (str): ì‚¬ìš©ì ì§ˆë¬¸
        model_interface: AI ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ (ë¡œì»¬/API/RAG ì§€ì›)
    """
    # ì‚¬ìš©ì ì§ˆë¬¸ì„ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
    st.session_state.chat_history.append({"role": "user", "content": question})
    
    # ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´ë¥¼ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    user_profile = get_user_profile()
    
    # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
    model_type = model_interface.get("type", "local") if model_interface else "local"
    
    if model_type == "rag":
        # RAG ëª¨ë¸ìš© ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ (RAG ì‹œìŠ¤í…œì´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ì¤Œ)
        enhanced_prompt = f"""As an OPIc English tutor, help with this question.

User Profile: {user_profile}

Question: {question}"""
    else:
        # ê¸°ì¡´ ë¡œì»¬ ëª¨ë¸ìš© ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸
        enhanced_prompt = f"""You are an OPIc (Oral Proficiency Interview-computer) English tutor. 

User Profile: {user_profile}

Based on the user's background information above, provide a helpful and encouraging response to their question. Keep your response conversational and supportive for English learning.

User Question: {question}

Response:"""
    
    # í”„ë¡¬í”„íŠ¸ í‘œì‹œ (ë””ë²„ê¹…ìš©)
    with st.expander("AI í”„ë¡¬í”„íŠ¸ (ê°œë°œììš©)"):
        st.code(enhanced_prompt)
        if model_interface:
            st.info(f"**ì‚¬ìš© ëª¨ë¸**: {model_type.upper()} - {model_interface.get('model_name', 'Unknown')}")
    
    # AI ì‘ë‹µ ìƒì„±
    with st.spinner("ğŸ¤– AIê°€ ìƒê° ì¤‘ì…ë‹ˆë‹¤..."):
        answer = generate_response(model_interface, enhanced_prompt)
        
        # ë¹ˆ ì‘ë‹µì¼ ê²½ìš° ê¸°ë³¸ ì‘ë‹µ ì œê³µ
        if not answer or answer.strip() in ["", "âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."]:
            answer = f"ì•ˆë…•í•˜ì„¸ìš”! ì§ˆë¬¸í•´ì£¼ì‹  '{question}'ì— ëŒ€í•´ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. OPIc ì¤€ë¹„ë¥¼ ë„ì™€ë“œë¦´ê²Œìš”! ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”."
    
    # AI ì‘ë‹µì„ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
    st.session_state.chat_history.append({"role": "bot", "content": answer})

def _render_chat_history():
    """ì±„íŒ… ê¸°ë¡ì„ ë Œë”ë§í•©ë‹ˆë‹¤."""
    for idx, msg in enumerate(st.session_state.chat_history):
        with st.chat_message("user" if msg["role"] == "user" else "assistant"):
            st.markdown(msg["content"])
            # AI ì‘ë‹µì—ë§Œ TTS ë²„íŠ¼ ì¶”ê°€
            if msg["role"] == "bot":
                display_tts_button(msg["content"], message_index=idx)
