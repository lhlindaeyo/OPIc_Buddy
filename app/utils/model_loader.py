"""
AI ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ - ë¡œì»¬/API/RAG ëª¨ë‘ ì§€ì›
"""
import streamlit as st
import os
import sys
from typing import Optional
from pathlib import Path

# í™˜ê²½ ë³€ìˆ˜ë¡œ ëª¨ë¸ íƒ€ì… ì„¤ì •
MODEL_TYPE = os.getenv("MODEL_TYPE", "rag")  # "local", "api", "rag"
API_KEY = os.getenv("OPENAI_API_KEY", "")

# RAG ê´€ë ¨ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

def load_model():
    """
    ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. 
    ë¡œì»¬ ëª¨ë¸, API, ë˜ëŠ” RAG ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if MODEL_TYPE == "api":
        return _load_api_client()
    elif MODEL_TYPE == "rag":
        return _load_rag_model()
    else:
        return _load_local_model()

@st.cache_resource
def _load_rag_model():
    """RAG + Phi ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        # RAG ëª¨ë“ˆ ë™ì  ì„í¬íŠ¸ (ìºì‹œëœ ë¦¬ì†ŒìŠ¤ì—ì„œ)
        from rag.rag import run_rag
        
        return {
            "type": "rag",
            "rag_function": run_rag,
            "model_name": "Phi-1.5 + E5 Embeddings"
        }
    except Exception as e:
        st.error(f"RAG ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        # Fallback to local model
        st.warning("RAG ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë¡œì»¬ ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        return _load_local_model()

@st.cache_resource
def _load_local_model():
    """ë¡œì»¬ FLAN-T5 ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
        tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
        model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
        return {
            "type": "local",
            "pipeline": pipeline("text2text-generation", model=model, tokenizer=tokenizer)
        }
    except Exception as e:
        st.error(f"ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def _load_api_client():
    """API í´ë¼ì´ì–¸íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. (í–¥í›„ OpenAI/ë‹¤ë¥¸ API ì—°ë™ìš©)"""
    return {
        "type": "api",
        "api_key": API_KEY,
        "model": "gpt-3.5-turbo"  # ë‚˜ì¤‘ì— ì‚¬ìš©í•  ëª¨ë¸
    }

def generate_response(model_interface: Optional[dict], prompt: str, max_tokens: int = 150) -> str:
    """
    í†µí•© ì‘ë‹µ ìƒì„± í•¨ìˆ˜ - ë¡œì»¬/API/RAG ëª¨ë‘ ì§€ì›
    
    Args:
        model_interface: ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ (ë¡œì»¬ íŒŒì´í”„ë¼ì¸, API ì„¤ì •, ë˜ëŠ” RAG í•¨ìˆ˜)
        prompt: ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
    
    Returns:
        ìƒì„±ëœ ì‘ë‹µ
    """
    if model_interface is None:
        return "âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    if model_interface["type"] == "api":
        return _generate_api_response(model_interface, prompt, max_tokens)
    elif model_interface["type"] == "rag":
        return _generate_rag_response(model_interface, prompt, max_tokens)
    else:
        return _generate_local_response(model_interface, prompt, max_tokens)

def _generate_rag_response(model_interface: dict, prompt: str, max_tokens: int) -> str:
    """RAG ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±"""
    try:
        rag_function = model_interface["rag_function"]
        
        # ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ì¶œ (í”„ë¡¬í”„íŠ¸ì—ì„œ ì‹¤ì œ ì§ˆë¬¸ ë¶€ë¶„ë§Œ ì¶”ì¶œ)
        user_question = prompt
        if "User Question:" in prompt:
            user_question = prompt.split("User Question:")[-1].strip()
        elif "Question:" in prompt:
            user_question = prompt.split("Question:")[-1].strip()
        
        # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ì‘ë‹µ ìƒì„± (k=3ê°œì˜ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰)
        response = rag_function(user_question, k=3)
        
        # ì‘ë‹µ ì •ì œ
        if not response or len(response.strip()) < 10:
            return "I'm here to help you practice English for OPIc! Could you please ask me something about English conversation or OPIc preparation?"
        
        return response.strip()
        
    except Exception as e:
        st.error(f"RAG ëª¨ë¸ ì˜¤ë¥˜: {e}")
        return f"I apologize for the technical issue. Could you please try asking your question again? (Error: {str(e)[:50]})"

def _generate_local_response(model_interface: dict, prompt: str, max_tokens: int) -> str:
    """ë¡œì»¬ ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±"""
    try:
        pipeline_model = model_interface["pipeline"]
        
        # ë” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë³€ê²½
        simple_prompt = f"Question: {prompt.split('User Question:')[-1].replace('Response:', '').strip()}\nAnswer:"
        
        response = pipeline_model(
            simple_prompt, 
            max_new_tokens=max_tokens, 
            do_sample=True, 
            temperature=0.7,
            repetition_penalty=1.2,  # ë°˜ë³µ ë°©ì§€
            num_beams=2
        )
        
        answer = response[0]["generated_text"].replace(simple_prompt, "").strip()
        
        # ë„ˆë¬´ ì§§ê±°ë‚˜ ë°˜ë³µë˜ëŠ” ë‹µë³€ í•„í„°ë§
        if not answer or len(answer) < 10 or answer.count(answer.split()[0] if answer.split() else "") > 3:
            return "Hello! I'm here to help you practice English for OPIc. Could you please ask me something specific about English conversation or OPIc preparation?"
        
        return answer
    except Exception as e:
        return f"I'm sorry, I had a technical issue. Could you please try asking your question again? (Error: {str(e)[:50]})"

def _generate_api_response(model_interface: dict, prompt: str, max_tokens: int) -> str:
    """APIë¡œ ì‘ë‹µ ìƒì„± (í–¥í›„ êµ¬í˜„ ì˜ˆì •)"""
    # TODO: OpenAI API ë˜ëŠ” ë‹¤ë¥¸ API ì—°ë™
    return "ğŸš§ API ëª¨ë“œëŠ” ì•„ì§ êµ¬í˜„ ì¤‘ì…ë‹ˆë‹¤. í˜„ì¬ëŠ” ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
