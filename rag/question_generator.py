"""
OPIc ì§ˆë¬¸ ìƒì„± ì „ìš© RAG ì‹œìŠ¤í…œ
"""
import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from langchain_community.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from transformers import pipeline
from langchain.chains import LLMChain
from retriever import retrieve_top_k
import torch

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
PHI15_MODEL_PATH = os.getenv("PHI15_MODEL_PATH", "microsoft/phi-1_5")

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ì— ë”°ë¼ device ì„¤ì •
device = 0 if torch.cuda.is_available() else -1

# ì§ˆë¬¸ ìƒì„±ìš© LLM ë¡œë“œ
try:
    question_gen_pipe = pipeline(
        "text-generation",
        model=PHI15_MODEL_PATH,
        tokenizer=PHI15_MODEL_PATH,
        device=device,
        temperature=0.3,
        max_new_tokens=100,
        do_sample=True
    )
    question_llm = HuggingFacePipeline(pipeline=question_gen_pipe)
    print(f"âœ… ì§ˆë¬¸ ìƒì„± ëª¨ë¸ ë¡œë“œ ì„±ê³µ (device: {'GPU' if device == 0 else 'CPU'})")
except Exception as e:
    print(f"âŒ ì§ˆë¬¸ ìƒì„± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    question_llm = None

# ì§ˆë¬¸ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ë” ê°„ë‹¨í•œ í˜•ì‹)
question_prompt_template = """Based on: {survey_context}

Generate a natural English interview question about {question_type}.

Example questions:
{context}

New question:"""

question_prompt = PromptTemplate(
    input_variables=["context", "survey_context", "question_type"],
    template=question_prompt_template
)

# Chain ìƒì„±
if question_llm is not None:
    try:
        question_chain = LLMChain(llm=question_llm, prompt=question_prompt)
        print("âœ… ì§ˆë¬¸ ìƒì„± Chain ìƒì„± ì„±ê³µ")
    except Exception as e:
        print(f"âŒ ì§ˆë¬¸ ìƒì„± Chain ìƒì„± ì‹¤íŒ¨: {e}")
        question_chain = None
else:
    question_chain = None


def generate_opic_question(survey_context: str, question_type: str, question_number: int, k: int = 3) -> str:
    """OPIc ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        if question_llm is None or question_chain is None:
            return None
        
        # 1) ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        search_query = f"OPIc question {question_type} {survey_context}"
        contexts = retrieve_top_k(search_query, k)
        
        if not contexts:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ì§ˆë¬¸ ìƒì„±
            contexts = [f"Generate an OPIc question about {question_type}"]
        
        # 2) ì»¨í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
        merged_context = "\n".join(contexts)
        
        # 3) ì§ˆë¬¸ ìƒì„±
        response = question_chain.run(
            context=merged_context[:200],  # ì»¨í…ìŠ¤íŠ¸ë¥¼ ì§§ê²Œ ì œí•œ
            survey_context=survey_context,
            question_type=question_type
        )
        
        # 4) ì‘ë‹µ ì •ì œ
        if response and len(response.strip()) > 10:
            # "Question:" ì œê±° ë° ì •ì œ
            cleaned_question = response.strip()
            if cleaned_question.startswith("Question:"):
                cleaned_question = cleaned_question[9:].strip()
            
            # ì¤„ë°”ê¿ˆê³¼ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
            cleaned_question = cleaned_question.split('\n')[0].strip()
            
            # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
            if len(cleaned_question) > 200:
                cleaned_question = cleaned_question[:200] + "..."
            
            return cleaned_question
            
    except Exception as e:
        print(f"ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")
        
    return None


def test_question_generation():
    """ì§ˆë¬¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
    test_cases = [
        {
            "survey_context": "living alone, watching sports, TV",
            "question_type": "survey_basic",
            "question_number": 2
        },
        {
            "survey_context": "movies, music, reading",
            "question_type": "survey_advanced",
            "question_number": 8
        }
    ]
    
    for i, case in enumerate(test_cases):
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i+1}:")
        print(f"Survey: {case['survey_context']}")
        print(f"Type: {case['question_type']}")
        
        question = generate_opic_question(
            case["survey_context"],
            case["question_type"],
            case["question_number"]
        )
        
        print(f"Generated Question: {question}")


if __name__ == "__main__":
    test_question_generation()
